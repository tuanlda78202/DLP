{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"9d58924cb4b812cfad8180f502cec804bf52981289113db4064943ea5da62a6a"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/torch-vision-references/references\")","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:52:26.533802Z","iopub.execute_input":"2022-12-30T07:52:26.534268Z","iopub.status.idle":"2022-12-30T07:52:26.560178Z","shell.execute_reply.started":"2022-12-30T07:52:26.534164Z","shell.execute_reply":"2022-12-30T07:52:26.559277Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install pycocotools\n!pip install GPUtil","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:52:26.562063Z","iopub.execute_input":"2022-12-30T07:52:26.563087Z","iopub.status.idle":"2022-12-30T07:52:46.116434Z","shell.execute_reply.started":"2022-12-30T07:52:26.563049Z","shell.execute_reply":"2022-12-30T07:52:46.115244Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pycocotools in /opt/conda/lib/python3.7/site-packages (2.0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pycocotools) (1.21.6)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from pycocotools) (3.5.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.3)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (9.1.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (21.3)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (4.33.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.1.0->pycocotools) (4.1.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.15.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: GPUtil in /opt/conda/lib/python3.7/site-packages (1.4.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom collections import namedtuple\nimport time\nfrom torch.nn import functional as F\nfrom tqdm import tqdm\n\nfrom torch import nn\nimport torch as t\nimport torchvision","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:52:46.118957Z","iopub.execute_input":"2022-12-30T07:52:46.119411Z","iopub.status.idle":"2022-12-30T07:52:48.125755Z","shell.execute_reply.started":"2022-12-30T07:52:46.119347Z","shell.execute_reply":"2022-12-30T07:52:48.124773Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom skimage import transform as sktsf\nfrom torchvision import transforms as tvtsf","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:52:48.131669Z","iopub.execute_input":"2022-12-30T07:52:48.134190Z","iopub.status.idle":"2022-12-30T07:52:49.074536Z","shell.execute_reply.started":"2022-12-30T07:52:48.134150Z","shell.execute_reply":"2022-12-30T07:52:49.073210Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from torchvision.datasets import VOCDetection\nimport numpy as np\nfrom numpy import random\n\nimport pandas as pd\nimport cv2\nimport os\nimport os.path as osp\nimport time\nimport random\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport torchvision.transforms.functional as FT\n\nfrom math import sqrt\nfrom sklearn import preprocessing \nfrom torch.utils.data import DataLoader, Dataset\n\nimport torch\nfrom torchvision import transforms\nimport types\nimport torchvision\nimport torch.utils.data as data\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nfrom PIL import Image, ImageDraw","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:52:49.079286Z","iopub.execute_input":"2022-12-30T07:52:49.081647Z","iopub.status.idle":"2022-12-30T07:52:49.383374Z","shell.execute_reply.started":"2022-12-30T07:52:49.081606Z","shell.execute_reply":"2022-12-30T07:52:49.382406Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"dataset = VOCDetection(root=r'/kaggle/input/d/tungbinhthuong/pascal-voc-2012/',year ='2012', image_set='train', download=False)\n\ntest_dataset = VOCDetection(root=r'/kaggle/input/d/tungbinhthuong/pascal-voc-2012/', year = '2012',image_set='val', download=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:52:49.384775Z","iopub.execute_input":"2022-12-30T07:52:49.385680Z","iopub.status.idle":"2022-12-30T07:52:49.443832Z","shell.execute_reply.started":"2022-12-30T07:52:49.385641Z","shell.execute_reply":"2022-12-30T07:52:49.442915Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def image_dict(split):\n    image_dict = {}\n    if split == \"train\":\n        for image_info in dataset:\n            image = image_info[0]\n            image_id = image_info[1]['annotation']['filename'].split(\".\")[0]\n            image_dict[image_id] = image\n    if split == \"test\":\n        for image_info in test_dataset:\n            image = image_info[0]\n            image_id = image_info[1]['annotation']['filename'].split(\".\")[0]\n            image_dict[image_id] = image\n    return image_dict","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:52:49.445224Z","iopub.execute_input":"2022-12-30T07:52:49.445598Z","iopub.status.idle":"2022-12-30T07:52:49.452094Z","shell.execute_reply.started":"2022-12-30T07:52:49.445561Z","shell.execute_reply":"2022-12-30T07:52:49.450940Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_image_dict = image_dict(\"train\")\ntest_image_dict = image_dict(\"test\")","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:52:49.453739Z","iopub.execute_input":"2022-12-30T07:52:49.454457Z","iopub.status.idle":"2022-12-30T07:55:31.822742Z","shell.execute_reply.started":"2022-12-30T07:52:49.454418Z","shell.execute_reply":"2022-12-30T07:55:31.821590Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class Parser(object):\n\n    def __init__(self,image):\n\n        self.image = image\n        self.annotations = self.image[1]['annotation']\n        self.objects = self.annotations[\"object\"]\n       \n        # image id \n        self.image_id = self.annotations['filename']\n        # names of the classes contained in the xml file\n        self.names = self._get_names()\n        # coordinates of the bounding boxes\n        self.boxes = self._get_bndbox()\n        # difficulties\n        self.difficulties = self._get_difficulties()\n\n    def _get_names(self):\n\n        names = []\n        for obj in self.objects:\n            name = obj['name']\n            names.append(name)\n        return np.array(names)\n\n    def _get_bndbox(self):\n\n        boxes = []\n        for obj in self.objects:\n            coordinates = []\n            bndbox = obj[\"bndbox\"]\n            coordinates.append(np.int32(bndbox[\"xmin\"]))\n            coordinates.append(np.int32(bndbox[\"ymin\"]))\n            coordinates.append(np.int32(bndbox[\"xmax\"]))\n            coordinates.append(np.int32(bndbox[\"ymax\"]))\n            boxes.append(coordinates)\n\n        return np.array(boxes)\n    \n    def _get_difficulties(self):\n        difficulties = []\n        for obj in self.objects:\n            difficult = obj['difficult']\n            difficulties.append(difficult)\n        return np.array(difficult)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:55:31.824409Z","iopub.execute_input":"2022-12-30T07:55:31.824801Z","iopub.status.idle":"2022-12-30T07:55:31.836380Z","shell.execute_reply.started":"2022-12-30T07:55:31.824760Z","shell.execute_reply":"2022-12-30T07:55:31.835422Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def inverse_normalize(img):\n    if opt.caffe_pretrain:\n        img = img + (np.array([122.7717, 115.9465, 102.9801]).reshape(3, 1, 1))\n        return img[::-1, :, :]\n    # approximate un-normalize for visualize\n    return (img * 0.225 + 0.45).clip(min=0, max=1) * 255\n\n\ndef pytorch_normalze(img):\n    \"\"\"\n    https://github.com/pytorch/vision/issues/223\n    return appr -1~1 RGB\n    \"\"\"\n    normalize = tvtsf.Normalize(mean=[0.485, 0.456, 0.406],\n                                std=[0.229, 0.224, 0.225])\n    img = normalize(t.from_numpy(img))\n    return img.numpy()\n\n\ndef caffe_normalize(img):\n    \"\"\"\n    return appr -125-125 BGR\n    \"\"\"\n    img = img[[2, 1, 0], :, :]  # RGB-BGR\n    img = img * 255\n    mean = np.array([122.7717, 115.9465, 102.9801]).reshape(3, 1, 1)\n    img = (img - mean).astype(np.float32, copy=True)\n    return img\n\n\ndef preprocess(img, min_size=600, max_size=1000):\n    \"\"\"Preprocess an image for feature extraction.\n\n    The length of the shorter edge is scaled to :obj:`self.min_size`.\n    After the scaling, if the length of the longer edge is longer than\n    :param min_size:\n    :obj:`self.max_size`, the image is scaled to fit the longer edge\n    to :obj:`self.max_size`.\n\n    After resizing the image, the image is subtracted by a mean image value\n    :obj:`self.mean`.\n\n    Args:\n        img (~numpy.ndarray): An image. This is in CHW and RGB format.\n            The range of its value is :math:`[0, 255]`.\n\n    Returns:\n        ~numpy.ndarray: A preprocessed image.\n\n    \"\"\"\n    C, H, W = img.shape\n    scale1 = min_size / min(H, W)\n    scale2 = max_size / max(H, W)\n    scale = min(scale1, scale2)\n    img = img / 255.\n    img = sktsf.resize(img, (C, H * scale, W * scale), mode='reflect',anti_aliasing=False)\n    # both the longer and shorter should be less than\n    # max_size and min_size\n    if opt['caffe_pretrain']:\n        normalize = caffe_normalize\n    else:\n        normalize = pytorch_normalze\n    return normalize(img)\n\n\nclass Transform(object):\n\n    def __init__(self, min_size=600, max_size=1000):\n        self.min_size = min_size\n        self.max_size = max_size\n\n    def __call__(self, in_data):\n        img, bbox, label = in_data\n        _, H, W = img.shape\n        img = preprocess(img, self.min_size, self.max_size)\n        _, o_H, o_W = img.shape\n        scale = o_H / H\n        bbox = util.resize_bbox(bbox, (H, W), (o_H, o_W))\n\n        # horizontally flip\n        img, params = util.random_flip(\n            img, x_random=True, return_param=True)\n        bbox = util.flip_bbox(\n            bbox, (o_H, o_W), x_flip=params['x_flip'])\n\n        return img, bbox, label, scale","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:55:31.841315Z","iopub.execute_input":"2022-12-30T07:55:31.842250Z","iopub.status.idle":"2022-12-30T07:55:31.857245Z","shell.execute_reply.started":"2022-12-30T07:55:31.842208Z","shell.execute_reply":"2022-12-30T07:55:31.856204Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def files_to_df(dataset):\n    \n    \"\"\"\"Return pandas dataframe from list of XML files.\"\"\"\n    \n    names = []\n    boxes = []\n    image_id = []\n    difficulties = []\n    for image in dataset:\n        parser = Parser(image)\n        names.extend(parser.names)\n        boxes.extend(parser.boxes)\n        image_id.extend([parser.image_id] * len(parser.names))\n        difficulties.extend([parser.difficulties])\n\n        # xml_path.extend([xml.xml_file] * len(xml.names))\n        # img_path.extend([xml.img_path] * len(xml.names))\n    a = {\"image_id\": image_id,\n         \"names\": names,\n         \"boxes\": boxes,\n         \"difficulties\": difficulties}\n    \n    df = pd.DataFrame.from_dict(a, orient='index')\n    df = df.transpose()\n    \n    return df\n\ndf_train = files_to_df(dataset)\ndf_test = files_to_df(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:55:31.859447Z","iopub.execute_input":"2022-12-30T07:55:31.860114Z","iopub.status.idle":"2022-12-30T07:56:40.693978Z","shell.execute_reply.started":"2022-12-30T07:55:31.860077Z","shell.execute_reply":"2022-12-30T07:56:40.692835Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def eval(dataloader, faster_rcnn, test_num=10000):\n    pred_bboxes, pred_labels, pred_scores = list(), list(), list()\n    gt_bboxes, gt_labels, gt_difficults = list(), list(), list()\n    for ii, (imgs, sizes, gt_bboxes_, gt_labels_, gt_difficults_) in tqdm(enumerate(dataloader)):\n        sizes = [sizes[0][0].item(), sizes[1][0].item()]\n        pred_bboxes_, pred_labels_, pred_scores_ = faster_rcnn.predict(imgs, [sizes])\n        gt_bboxes += list(gt_bboxes_.numpy())\n        gt_labels += list(gt_labels_.numpy())\n        gt_difficults += list(gt_difficults_.numpy())\n        pred_bboxes += pred_bboxes_\n        pred_labels += pred_labels_\n        pred_scores += pred_scores_\n        if ii == test_num: break\n\n    result = eval_detection_voc(\n        pred_bboxes, pred_labels, pred_scores,\n        gt_bboxes, gt_labels, gt_difficults,\n        use_07_metric=True)\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:56:40.695618Z","iopub.execute_input":"2022-12-30T07:56:40.696006Z","iopub.status.idle":"2022-12-30T07:56:40.704442Z","shell.execute_reply.started":"2022-12-30T07:56:40.695968Z","shell.execute_reply":"2022-12-30T07:56:40.703293Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# remove .jpg extension from image_id \ndf_train['img_id'] = df_train['image_id'].apply(lambda x:x.split('.')).map(lambda x:x[0])\ndf_train.drop(columns=['image_id'], inplace=True)\n\ndf_test['img_id'] = df_test['image_id'].apply(lambda x:x.split('.')).map(lambda x:x[0])\ndf_test.drop(columns=['image_id'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:56:40.706094Z","iopub.execute_input":"2022-12-30T07:56:40.706472Z","iopub.status.idle":"2022-12-30T07:56:40.753173Z","shell.execute_reply.started":"2022-12-30T07:56:40.706428Z","shell.execute_reply":"2022-12-30T07:56:40.752277Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# classes need to be in int form so we use LabelEncoder for this task\nenc = preprocessing.LabelEncoder()\ndf_train['labels'] = enc.fit_transform(df_train['names'])\ndf_train['labels'] = np.stack(df_train['labels'][i]+1 for i in range(len(df_train['labels'])))\n\ndf_test['labels'] = enc.fit_transform(df_test['names'])\ndf_test['labels'] = np.stack(df_test['labels'][i]+1 for i in range(len(df_test['labels'])))","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:56:40.754381Z","iopub.execute_input":"2022-12-30T07:56:40.754746Z","iopub.status.idle":"2022-12-30T07:56:40.979695Z","shell.execute_reply.started":"2022-12-30T07:56:40.754699Z","shell.execute_reply":"2022-12-30T07:56:40.978682Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3472: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n  if (await self.run_code(code, result,  async_=asy)):\n","output_type":"stream"}]},{"cell_type":"code","source":"# make dictionary for class objects so we can call objects by their keys.\nrev_label_map= {0: 'background',\n                1:'aeroplane',\n              2:'bicycle',\n              3:'bird',\n              4:'boat',\n              5:'bottle',\n              6:'bus',\n              7:'car',\n              8:'cat',\n              9:'chair',\n              10:'cow',\n              11:'diningtable',\n              12:'dog',\n              13:'horse',\n              14:'motorbike',\n              15:'person',\n              16:'pottedplant',\n              17:'sheep',\n              18:'sofa',\n              19:'train',\n              20:'tvmonitor'}\n\nlabel_map = {v: k for k, v in rev_label_map.items()}\nlabel_map","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:56:40.981043Z","iopub.execute_input":"2022-12-30T07:56:40.982037Z","iopub.status.idle":"2022-12-30T07:56:40.995119Z","shell.execute_reply.started":"2022-12-30T07:56:40.981997Z","shell.execute_reply":"2022-12-30T07:56:40.994089Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"{'background': 0,\n 'aeroplane': 1,\n 'bicycle': 2,\n 'bird': 3,\n 'boat': 4,\n 'bottle': 5,\n 'bus': 6,\n 'car': 7,\n 'cat': 8,\n 'chair': 9,\n 'cow': 10,\n 'diningtable': 11,\n 'dog': 12,\n 'horse': 13,\n 'motorbike': 14,\n 'person': 15,\n 'pottedplant': 16,\n 'sheep': 17,\n 'sofa': 18,\n 'train': 19,\n 'tvmonitor': 20}"},"metadata":{}}]},{"cell_type":"code","source":"# bounding box coordinates point need to be in separate columns\n\ndf_train['xmin'] = -1\ndf_train['ymin'] = -1\ndf_train['xmax'] = -1\ndf_train['ymax'] = -1\n\ndf_train[['xmin','ymin','xmax','ymax']]=np.stack(df_train['boxes'][i] for i in range(len(df_train['boxes'])))\n\ndf_train.drop(columns=['boxes'], inplace=True)\ndf_train['xmin'] = df_train['xmin'].astype(np.float32)\ndf_train['ymin'] = df_train['ymin'].astype(np.float32)\ndf_train['xmax'] = df_train['xmax'].astype(np.float32)\ndf_train['ymax'] = df_train['ymax'].astype(np.float32)\ndf_train['difficulties'] = df_train['difficulties'].astype(np.float32)\n\ndf_test['xmin'] = -1\ndf_test['ymin'] = -1\ndf_test['xmax'] = -1\ndf_test['ymax'] = -1\n\ndf_test[['xmin','ymin','xmax','ymax']]=np.stack(df_test['boxes'][i] for i in range(len(df_test['boxes'])))\n\ndf_test.drop(columns=['boxes'], inplace=True)\ndf_test['xmin'] = df_test['xmin'].astype(np.float32)\ndf_test['ymin'] = df_test['ymin'].astype(np.float32)\ndf_test['xmax'] = df_test['xmax'].astype(np.float32)\ndf_test['ymax'] = df_test['ymax'].astype(np.float32)\ndf_test['difficulties'] = df_test['difficulties'].astype(np.float32)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:56:40.996865Z","iopub.execute_input":"2022-12-30T07:56:40.997243Z","iopub.status.idle":"2022-12-30T07:56:41.376286Z","shell.execute_reply.started":"2022-12-30T07:56:40.997197Z","shell.execute_reply":"2022-12-30T07:56:41.375253Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"df_train.drop(columns=['names'], inplace=True)\ndf_test.drop(columns=['names'], inplace=True)\n\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:56:41.378296Z","iopub.execute_input":"2022-12-30T07:56:41.378944Z","iopub.status.idle":"2022-12-30T07:56:41.402516Z","shell.execute_reply.started":"2022-12-30T07:56:41.378903Z","shell.execute_reply":"2022-12-30T07:56:41.401433Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"   difficulties       img_id  labels   xmin   ymin   xmax   ymax\n0           0.0  2008_000002      20   34.0   11.0  448.0  293.0\n1           0.0  2008_000003      19   46.0   11.0  500.0  333.0\n2           0.0  2008_000003      15   62.0  190.0   83.0  243.0\n3           0.0  2008_000007       4    1.0  230.0  428.0  293.0\n4           0.0  2008_000009      10  217.0  161.0  294.0  221.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>difficulties</th>\n      <th>img_id</th>\n      <th>labels</th>\n      <th>xmin</th>\n      <th>ymin</th>\n      <th>xmax</th>\n      <th>ymax</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>2008_000002</td>\n      <td>20</td>\n      <td>34.0</td>\n      <td>11.0</td>\n      <td>448.0</td>\n      <td>293.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>2008_000003</td>\n      <td>19</td>\n      <td>46.0</td>\n      <td>11.0</td>\n      <td>500.0</td>\n      <td>333.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>2008_000003</td>\n      <td>15</td>\n      <td>62.0</td>\n      <td>190.0</td>\n      <td>83.0</td>\n      <td>243.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>2008_000007</td>\n      <td>4</td>\n      <td>1.0</td>\n      <td>230.0</td>\n      <td>428.0</td>\n      <td>293.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>2008_000009</td>\n      <td>10</td>\n      <td>217.0</td>\n      <td>161.0</td>\n      <td>294.0</td>\n      <td>221.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def find_intersection(set_1, set_2):\n    \"\"\"\n    Find the intersection of every box combination between two sets of boxes that are in boundary coordinates.\n    :param set_1: set 1, a tensor of dimensions (n1, 4)\n    :param set_2: set 2, a tensor of dimensions (n2, 4)\n    :return: intersection of each of the boxes in set 1 with respect to each of the boxes in set 2, a tensor of dimensions (n1, n2)\n    \"\"\"\n\n    # PyTorch auto-broadcasts singleton dimensions\n    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))  # (n1, n2, 2)\n    upper_bounds = torch.min(set_1[:, 2:].unsqueeze(1), set_2[:, 2:].unsqueeze(0))  # (n1, n2, 2)\n    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)  # (n1, n2, 2)\n    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1]  # (n1, n2)\n\n\ndef find_jaccard_overlap(set_1, set_2):\n    \"\"\"\n    Find the Jaccard Overlap (IoU) of every box combination between two sets of boxes that are in boundary coordinates.\n    :param set_1: set 1, a tensor of dimensions (n1, 4)\n    :param set_2: set 2, a tensor of dimensions (n2, 4)\n    :return: Jaccard Overlap of each of the boxes in set 1 with respect to each of the boxes in set 2, a tensor of dimensions (n1, n2)\n    \"\"\"\n\n    # Find intersections\n    intersection = find_intersection(set_1, set_2)  # (n1, n2)\n\n    # Find areas of each box in both sets\n    areas_set_1 = (set_1[:, 2] - set_1[:, 0]) * (set_1[:, 3] - set_1[:, 1])  # (n1)\n    areas_set_2 = (set_2[:, 2] - set_2[:, 0]) * (set_2[:, 3] - set_2[:, 1])  # (n2)\n\n    # Find the union\n    # PyTorch auto-broadcasts singleton dimensions\n    union = areas_set_1.unsqueeze(1) + areas_set_2.unsqueeze(0) - intersection  # (n1, n2)\n\n    return intersection / union  # (n1, n2)\n\n\n# Some augmentation functions below have been adapted from\n# From https://github.com/amdegroot/ssd.pytorch/blob/master/utils/augmentations.py\n\ndef expand(image, boxes, filler):\n    \"\"\"\n    Perform a zooming out operation by placing the image in a larger canvas of filler material.\n    Helps to learn to detect smaller objects.\n    :param image: image, a tensor of dimensions (3, original_h, original_w)\n    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n    :param filler: RBG values of the filler material, a list like [R, G, B]\n    :return: expanded image, updated bounding box coordinates\n    \"\"\"\n    # Calculate dimensions of proposed expanded (zoomed-out) image\n    original_h = image.size(1)\n    original_w = image.size(2)\n    max_scale = 4\n    scale = random.uniform(1, max_scale)\n    new_h = int(scale * original_h)\n    new_w = int(scale * original_w)\n\n    # Create such an image with the filler\n    filler = torch.FloatTensor(filler)  # (3)\n    new_image = torch.ones((3, new_h, new_w), dtype=torch.float) * filler.unsqueeze(1).unsqueeze(1)  # (3, new_h, new_w)\n    # Note - do not use expand() like new_image = filler.unsqueeze(1).unsqueeze(1).expand(3, new_h, new_w)\n    # because all expanded values will share the same memory, so changing one pixel will change all\n\n    # Place the original image at random coordinates in this new image (origin at top-left of image)\n    left = random.randint(0, new_w - original_w)\n    right = left + original_w\n    top = random.randint(0, new_h - original_h)\n    bottom = top + original_h\n    new_image[:, top:bottom, left:right] = image\n\n    # Adjust bounding boxes' coordinates accordingly\n    new_boxes = boxes + torch.FloatTensor([left, top, left, top]).unsqueeze(\n        0)  # (n_objects, 4), n_objects is the no. of objects in this image\n\n    return new_image, new_boxes\n\n\ndef random_crop(image, boxes, labels, difficulties):\n    \"\"\"\n    Performs a random crop in the manner stated in the paper. Helps to learn to detect larger and partial objects.\n    Note that some objects may be cut out entirely.\n    Adapted from https://github.com/amdegroot/ssd.pytorch/blob/master/utils/augmentations.py\n    :param image: image, a tensor of dimensions (3, original_h, original_w)\n    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n    :param labels: labels of objects, a tensor of dimensions (n_objects)\n    :param difficulties: difficulties of detection of these objects, a tensor of dimensions (n_objects)\n    :return: cropped image, updated bounding box coordinates, updated labels, updated difficulties\n    \"\"\"\n    original_h = image.size(1)\n    original_w = image.size(2)\n    # Keep choosing a minimum overlap until a successful crop is made\n    while True:\n        # Randomly draw the value for minimum overlap\n        min_overlap = random.choice([0., .1, .3, .5, .7, .9, None])  # 'None' refers to no cropping\n\n        # If not cropping\n        if min_overlap is None:\n            return image, boxes, labels, difficulties\n\n        # Try up to 50 times for this choice of minimum overlap\n        # This isn't mentioned in the paper, of course, but 50 is chosen in paper authors' original Caffe repo\n        max_trials = 50\n        for _ in range(max_trials):\n            # Crop dimensions must be in [0.3, 1] of original dimensions\n            # Note - it's [0.1, 1] in the paper, but actually [0.3, 1] in the authors' repo\n            min_scale = 0.3\n            scale_h = random.uniform(min_scale, 1)\n            scale_w = random.uniform(min_scale, 1)\n            new_h = int(scale_h * original_h)\n            new_w = int(scale_w * original_w)\n\n            # Aspect ratio has to be in [0.5, 2]\n            aspect_ratio = new_h / new_w\n            if not 0.5 < aspect_ratio < 2:\n                continue\n\n            # Crop coordinates (origin at top-left of image)\n            left = random.randint(0, original_w - new_w)\n            right = left + new_w\n            top = random.randint(0, original_h - new_h)\n            bottom = top + new_h\n            crop = torch.FloatTensor([left, top, right, bottom])  # (4)\n\n            # Calculate Jaccard overlap between the crop and the bounding boxes\n            overlap = find_jaccard_overlap(crop.unsqueeze(0),\n                                           boxes)  # (1, n_objects), n_objects is the no. of objects in this image\n            overlap = overlap.squeeze(0)  # (n_objects)\n\n            # If not a single bounding box has a Jaccard overlap of greater than the minimum, try again\n            if overlap.max().item() < min_overlap:\n                continue\n\n            # Crop image\n            new_image = image[:, top:bottom, left:right]  # (3, new_h, new_w)\n\n            # Find centers of original bounding boxes\n            bb_centers = (boxes[:, :2] + boxes[:, 2:]) / 2.  # (n_objects, 2)\n\n            # Find bounding boxes whose centers are in the crop\n            centers_in_crop = (bb_centers[:, 0] > left) * (bb_centers[:, 0] < right) * (bb_centers[:, 1] > top) * (\n                    bb_centers[:, 1] < bottom)  # (n_objects), a Torch uInt8/Byte tensor, can be used as a boolean index\n\n            # If not a single bounding box has its center in the crop, try again\n            if not centers_in_crop.any():\n                continue\n\n            # Discard bounding boxes that don't meet this criterion\n            new_boxes = boxes[centers_in_crop, :]\n            new_labels = labels[centers_in_crop]\n            new_difficulties = difficulties[centers_in_crop]\n\n            # Calculate bounding boxes' new coordinates in the crop\n            new_boxes[:, :2] = torch.max(new_boxes[:, :2], crop[:2])  # crop[:2] is [left, top]\n            new_boxes[:, :2] -= crop[:2]\n            new_boxes[:, 2:] = torch.min(new_boxes[:, 2:], crop[2:])  # crop[2:] is [right, bottom]\n            new_boxes[:, 2:] -= crop[:2]\n\n            return new_image, new_boxes, new_labels, new_difficulties\n\n\ndef flip(image, boxes):\n    \"\"\"\n    Flip image horizontally.\n    :param image: image, a PIL Image\n    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n    :return: flipped image, updated bounding box coordinates\n    \"\"\"\n    # Flip image\n    new_image = FT.hflip(image)\n\n    # Flip boxes\n    new_boxes = boxes\n    new_boxes[:, 0] = image.width - boxes[:, 0] - 1\n    new_boxes[:, 2] = image.width - boxes[:, 2] - 1\n    new_boxes = new_boxes[:, [2, 1, 0, 3]]\n\n    return new_image, new_boxes\n\n\ndef resize(image, boxes, dims=(300, 300), return_percent_coords=True):\n    \"\"\"\n    Resize image. For the SSD300, resize to (300, 300).\n    Since percent/fractional coordinates are calculated for the bounding boxes (w.r.t image dimensions) in this process,\n    you may choose to retain them.\n    :param image: image, a PIL Image\n    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n    :return: resized image, updated bounding box coordinates (or fractional coordinates, in which case they remain the same)\n    \"\"\"\n    # Resize image\n    new_image = FT.resize(image, dims)\n\n    # Resize bounding boxes\n    old_dims = torch.FloatTensor([image.width, image.height, image.width, image.height]).unsqueeze(0)\n    new_boxes = boxes / old_dims  # percent coordinates\n\n    if not return_percent_coords:\n        new_dims = torch.FloatTensor([dims[1], dims[0], dims[1], dims[0]]).unsqueeze(0)\n        new_boxes = new_boxes * new_dims\n\n    return new_image, new_boxes\n\n\ndef photometric_distort(image):\n    \"\"\"\n    Distort brightness, contrast, saturation, and hue, each with a 50% chance, in random order.\n    :param image: image, a PIL Image\n    :return: distorted image\n    \"\"\"\n    new_image = image\n\n    distortions = [FT.adjust_brightness,\n                   FT.adjust_contrast,\n                   FT.adjust_saturation,\n                   FT.adjust_hue]\n\n    random.shuffle(distortions)\n\n    for d in distortions:\n        if random.random() < 0.5:\n            if d.__name__ is 'adjust_hue':\n                # Caffe repo uses a 'hue_delta' of 18 - we divide by 255 because PyTorch needs a normalized value\n                adjust_factor = random.uniform(-18 / 255., 18 / 255.)\n            else:\n                # Caffe repo uses 'lower' and 'upper' values of 0.5 and 1.5 for brightness, contrast, and saturation\n                adjust_factor = random.uniform(0.5, 1.5)\n\n            # Apply this distortion\n            new_image = d(new_image, adjust_factor)\n\n    return new_image\n\n\ndef transform(image, boxes, labels, difficulties, split):\n    \"\"\"\n    Apply the transformations above.\n    :param image: image, a PIL Image\n    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n    :param labels: labels of objects, a tensor of dimensions (n_objects)\n    :param difficulties: difficulties of detection of these objects, a tensor of dimensions (n_objects)\n    :param split: one of 'TRAIN' or 'TEST', since different sets of transformations are applied\n    :return: transformed image, transformed bounding box coordinates, transformed labels, transformed difficulties\n    \"\"\"\n    assert split in {'TRAIN', 'TEST'}\n\n    # Mean and standard deviation of ImageNet data that our base VGG from torchvision was trained on\n    # see: https://pytorch.org/docs/stable/torchvision/models.html\n    mean = [0.485, 0.456, 0.406]\n    std = [0.229, 0.224, 0.225]\n\n    new_image = image\n    new_boxes = boxes\n    new_labels = labels\n    new_difficulties = difficulties\n    # Skip the following operations for evaluation/testing\n    if split == 'TRAIN':\n        # A series of photometric distortions in random order, each with 50% chance of occurrence, as in Caffe repo\n        new_image = photometric_distort(new_image)\n\n        # Convert PIL image to Torch tensor\n        new_image = FT.to_tensor(new_image)\n\n        # Expand image (zoom out) with a 50% chance - helpful for training detection of small objects\n        # Fill surrounding space with the mean of ImageNet data that our base VGG was trained on\n        if random.random() < 0.5:\n            new_image, new_boxes = expand(new_image, boxes, filler=mean)\n\n        # Randomly crop image (zoom in)\n        new_image, new_boxes, new_labels, new_difficulties = random_crop(new_image, new_boxes, new_labels,\n                                                                         new_difficulties)\n\n        # Convert Torch tensor to PIL image\n        new_image = FT.to_pil_image(new_image)\n\n        # Flip image with a 50% chance\n        if random.random() < 0.5:\n            new_image, new_boxes = flip(new_image, new_boxes)\n\n    # Resize image to (300, 300) - this also converts absolute boundary coordinates to their fractional form\n    new_image, new_boxes = resize(new_image, new_boxes, dims=(300, 300))\n\n    # Convert PIL image to Torch tensor\n    new_image = FT.to_tensor(new_image)\n\n    # Normalize by mean and standard deviation of ImageNet data that our base VGG was trained on\n    new_image = FT.normalize(new_image, mean=mean, std=std)\n\n    return new_image, new_boxes, new_labels, new_difficulties\n\n\ndef adjust_learning_rate(optimizer, scale):\n    \"\"\"\n    Scale learning rate by a specified factor.\n    :param optimizer: optimizer whose learning rate must be shrunk.\n    :param scale: factor to multiply learning rate with.\n    \"\"\"\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = param_group['lr'] * scale\n    print(\"DECAYING learning rate.\\n The new LR is %f\\n\" % (optimizer.param_groups[1]['lr'],))","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:56:41.404499Z","iopub.execute_input":"2022-12-30T07:56:41.404889Z","iopub.status.idle":"2022-12-30T07:56:41.440963Z","shell.execute_reply.started":"2022-12-30T07:56:41.404851Z","shell.execute_reply":"2022-12-30T07:56:41.439772Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class VOCDataset(Dataset):\n    \n    def __init__(self, dataframe, image_dict, split, keep_difficult=False):\n        super().__init__()\n        \n        self.image_ids = dataframe['img_id'].unique()\n        self.df = dataframe\n        self.image_dict = image_dict\n        self.transforms = transforms\n        self.split = split\n    \n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        records = self.df[self.df['img_id'] == image_id]\n        \n        image = self.image_dict[image_id]\n        image = image.convert(\"RGB\")\n        \n        boxes = records[['xmin', 'ymin', 'xmax','ymax']].values\n        boxes = torch.FloatTensor(records[['xmin', 'ymin', 'xmax','ymax']].values)\n        \n        labels = records['labels'].values\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n\n        difficulties = records[\"difficulties\"].values\n        difficulties = torch.as_tensor(difficulties, dtype=torch.int32)\n        \n#         boxes = self.box_resize(boxes, image)\n        seed = np.int64(42) # make a seed with numpy generator \n        random.seed(seed)\n        torch.manual_seed(seed)\n        # Apply transformations\n        image, boxes, labels, difficulties = transform(image, boxes, labels, difficulties, split=self.split) \n        \n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"image_id\"] = torch.tensor(float(image_id), dtype=torch.float32)\n        target[\"difficulties\"] = difficulties\n        \n        return image, target\n    \n\n    def box_resize(self, box, img, dims=(300, 300)):\n        old_dims = torch.FloatTensor([img.width, img.height, img.width, img.height]).unsqueeze(0)\n        new_box = box / old_dims\n        new_dims = torch.FloatTensor([dims[1], dims[0], dims[1], dims[0]]).unsqueeze(0)        \n        return new_box   \n    \n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n    \n    def collate_fn(self, batch):\n        \"\"\"\n        Since each image may have a different number of objects, we need a collate function (to be passed to the DataLoader).\n        This describes how to combine these tensors of different sizes. We use lists.\n        Note: this need not be defined in this Class, can be standalone.\n        :param batch: an iterable of N sets from __getitem__()\n        :return: a tensor of images, lists of varying-size tensors of bounding boxes, labels, and difficulties\n        \"\"\"\n        images = list()\n        boxes = list()\n        labels = list()\n        difficulties = list()\n\n        for b in batch:\n            images.append(b[0])\n            boxes.append(b[1])\n            labels.append(b[2])\n            difficulties.append(b[3])\n\n        images = torch.stack(images, dim=0)\n\n        return images, boxes, labels, difficulties  # tensor (N, 3, 300, 300), 3 lists of N tensors each","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:56:41.442486Z","iopub.execute_input":"2022-12-30T07:56:41.443125Z","iopub.status.idle":"2022-12-30T07:56:41.458007Z","shell.execute_reply.started":"2022-12-30T07:56:41.443083Z","shell.execute_reply":"2022-12-30T07:56:41.457000Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:56:41.460438Z","iopub.execute_input":"2022-12-30T07:56:41.460883Z","iopub.status.idle":"2022-12-30T07:56:41.482900Z","shell.execute_reply.started":"2022-12-30T07:56:41.460847Z","shell.execute_reply":"2022-12-30T07:56:41.481829Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"   difficulties       img_id  labels   xmin  ymin   xmax   ymax\n0           0.0  2008_000008      13   53.0  87.0  471.0  420.0\n1           0.0  2008_000008      15  158.0  44.0  289.0  167.0\n2           0.0  2008_000015       5  270.0   1.0  378.0  176.0\n3           0.0  2008_000015       5   57.0   1.0  164.0  150.0\n4           0.0  2008_000019      12  139.0   2.0  372.0  197.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>difficulties</th>\n      <th>img_id</th>\n      <th>labels</th>\n      <th>xmin</th>\n      <th>ymin</th>\n      <th>xmax</th>\n      <th>ymax</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>2008_000008</td>\n      <td>13</td>\n      <td>53.0</td>\n      <td>87.0</td>\n      <td>471.0</td>\n      <td>420.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>2008_000008</td>\n      <td>15</td>\n      <td>158.0</td>\n      <td>44.0</td>\n      <td>289.0</td>\n      <td>167.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>2008_000015</td>\n      <td>5</td>\n      <td>270.0</td>\n      <td>1.0</td>\n      <td>378.0</td>\n      <td>176.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>2008_000015</td>\n      <td>5</td>\n      <td>57.0</td>\n      <td>1.0</td>\n      <td>164.0</td>\n      <td>150.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>2008_000019</td>\n      <td>12</td>\n      <td>139.0</td>\n      <td>2.0</td>\n      <td>372.0</td>\n      <td>197.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"keep_difficult = True\ntrain_dataset = VOCDataset(df_train, train_image_dict, \"TRAIN\",keep_difficult)\ntest_dataset = VOCDataset(df_test, test_image_dict, \"TEST\",keep_difficult)\n\ndel train_image_dict, test_image_dict\ndel df_train, df_test","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:56:41.484788Z","iopub.execute_input":"2022-12-30T07:56:41.485273Z","iopub.status.idle":"2022-12-30T07:56:41.497900Z","shell.execute_reply.started":"2022-12-30T07:56:41.485236Z","shell.execute_reply":"2022-12-30T07:56:41.496544Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import utils","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:56:41.499967Z","iopub.execute_input":"2022-12-30T07:56:41.500459Z","iopub.status.idle":"2022-12-30T07:56:41.516164Z","shell.execute_reply.started":"2022-12-30T07:56:41.500418Z","shell.execute_reply":"2022-12-30T07:56:41.515242Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# split the dataset in train and test set\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=16,\n    shuffle=True,\n    collate_fn=utils.collate_fn\n)\n\ntest_data_loader = DataLoader(\n    test_dataset,\n    batch_size=1,\n    shuffle=False,\n    collate_fn=utils.collate_fn\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:56:41.519074Z","iopub.execute_input":"2022-12-30T07:56:41.519344Z","iopub.status.idle":"2022-12-30T07:56:41.524598Z","shell.execute_reply.started":"2022-12-30T07:56:41.519317Z","shell.execute_reply":"2022-12-30T07:56:41.523262Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"print(\"Number of training iterations: \", len(train_data_loader))\nprint(\"Number of testing iterations: \", len(test_data_loader))","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:56:41.526409Z","iopub.execute_input":"2022-12-30T07:56:41.526817Z","iopub.status.idle":"2022-12-30T07:56:41.535971Z","shell.execute_reply.started":"2022-12-30T07:56:41.526782Z","shell.execute_reply":"2022-12-30T07:56:41.534903Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Number of training iterations:  358\nNumber of testing iterations:  5823\n","output_type":"stream"}]},{"cell_type":"code","source":"from torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n# load a model\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained = False)\n\n# replace the classifier with a new one, that has\n# num_classes which is user-defined\nnum_classes = len(label_map)  # 1 class (person) + background\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:56:41.537869Z","iopub.execute_input":"2022-12-30T07:56:41.538301Z","iopub.status.idle":"2022-12-30T07:56:42.832869Z","shell.execute_reply.started":"2022-12-30T07:56:41.538258Z","shell.execute_reply":"2022-12-30T07:56:42.831828Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(epoch, model, optimizer):\n    \"\"\"\n    Save model checkpoint.\n    :param epoch: epoch number\n    :param model: model\n    :param optimizer: optimizer\n    \"\"\"\n    state = {'epoch': epoch,\n             'model': model,\n             'optimizer': optimizer}\n    filename = 'checkpoint_frcnn.pth.tar'\n    torch.save(state, filename)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:56:42.834264Z","iopub.execute_input":"2022-12-30T07:56:42.834657Z","iopub.status.idle":"2022-12-30T07:56:42.840638Z","shell.execute_reply.started":"2022-12-30T07:56:42.834618Z","shell.execute_reply":"2022-12-30T07:56:42.839631Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"from engine import train_one_epoch, evaluate\n\n# Data parameters\ndata_folder = '/kaggle/input/d/tungbinhthuong/pascal-voc-2012/'  # folder with data files\n\n# Model parameters\n# Not too many here since the SSD300 has a very specific structure\nn_classes = len(label_map)  # number of different types of objects\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Learning parameters\ncheckpoint = '/kaggle/working/checkpoint_frcnn_resnet.pth.tar'\n  # path to model checkpoint, None if none\n# batch_size = 16  # batch size\niterations = 120000  # number of iterations to train\n# workers = 4  # number of workers for loading data in the DataLoader\nprint_freq = 50  # print training status every __ batches\nlr = 1e-3  # learning rate\ndecay_lr_at = [80000, 100000]  # decay learning rate after these many iterations\ndecay_lr_to = 0.1  # decay learning rate to this fraction of the existing learning rate\nmomentum = 0.9  # momentum\nweight_decay = 5e-4  # weight decay\ngrad_clip = True  # clip if gradients are exploding, which may happen at larger batch sizes (sometimes at 32) - you will recognize it by a sorting error in the MuliBox loss calculation\n\ndef train(model, data_loader, data_loader_test):\n    # move model to the right device\n    model.to(device)\n\n    # construct an optimizer\n    params = [p for p in model.parameters() if p.requires_grad]\n    optimizer = torch.optim.SGD(params, lr=0.005,\n                                momentum=0.9, weight_decay=0.0005)\n    # and a learning rate scheduler\n    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                                   step_size=3,\n                                                   gamma=0.1)\n\n    # let's train it for 100 epochs\n    num_epochs = 1\n\n    for epoch in range(num_epochs):\n        # train for one epoch, printing every 10 iterations\n        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n        gc.collect()\n        torch.cuda.empty_cache()\n        # update the learning rate\n        lr_scheduler.step()\n        # evaluate on the test dataset\n#         evaluate(model, data_loader_test, device=device)\n        save_checkpoint(epoch, model, optimizer)\n    print(\"That's it!\")","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:56:42.842244Z","iopub.execute_input":"2022-12-30T07:56:42.842933Z","iopub.status.idle":"2022-12-30T07:56:42.974735Z","shell.execute_reply.started":"2022-12-30T07:56:42.842892Z","shell.execute_reply":"2022-12-30T07:56:42.973819Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"train(model, train_data_loader, test_data_loader)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T07:56:42.980237Z","iopub.execute_input":"2022-12-30T07:56:42.981616Z","iopub.status.idle":"2022-12-30T08:00:05.232413Z","shell.execute_reply.started":"2022-12-30T07:56:42.981574Z","shell.execute_reply":"2022-12-30T08:00:05.229451Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Epoch: [0]  [  0/358]  eta: 0:51:35  lr: 0.000019  loss: 4.8191 (4.8191)  loss_classifier: 3.1017 (3.1017)  loss_box_reg: 0.0008 (0.0008)  loss_objectness: 0.7169 (0.7169)  loss_rpn_box_reg: 0.9996 (0.9996)  time: 8.6475  data: 0.3189  max mem: 11294\nEpoch: [0]  [ 10/358]  eta: 0:15:44  lr: 0.000159  loss: 4.2899 (3.9398)  loss_classifier: 2.3939 (2.1382)  loss_box_reg: 0.0011 (0.0016)  loss_objectness: 0.7108 (0.7068)  loss_rpn_box_reg: 1.0709 (1.0932)  time: 2.7153  data: 0.3418  max mem: 11558\nEpoch: [0]  [ 20/358]  eta: 0:13:47  lr: 0.000299  loss: 2.3835 (3.0858)  loss_classifier: 0.6490 (1.3353)  loss_box_reg: 0.0023 (0.0034)  loss_objectness: 0.6759 (0.6319)  loss_rpn_box_reg: 1.1194 (1.1152)  time: 2.1372  data: 0.3573  max mem: 11558\nEpoch: [0]  [ 30/358]  eta: 0:12:45  lr: 0.000439  loss: 1.5268 (2.5214)  loss_classifier: 0.2405 (0.9672)  loss_box_reg: 0.0097 (0.0078)  loss_objectness: 0.4131 (0.5386)  loss_rpn_box_reg: 0.9071 (1.0078)  time: 2.1251  data: 0.3475  max mem: 11558\nEpoch: [0]  [ 40/358]  eta: 0:12:03  lr: 0.000579  loss: 1.0400 (2.1382)  loss_classifier: 0.1614 (0.7652)  loss_box_reg: 0.0160 (0.0093)  loss_objectness: 0.2837 (0.4705)  loss_rpn_box_reg: 0.6302 (0.8931)  time: 2.0935  data: 0.3250  max mem: 11558\nEpoch: [0]  [ 50/358]  eta: 0:11:30  lr: 0.000719  loss: 0.7395 (1.8324)  loss_classifier: 0.0915 (0.6310)  loss_box_reg: 0.0095 (0.0090)  loss_objectness: 0.2024 (0.4078)  loss_rpn_box_reg: 0.4101 (0.7847)  time: 2.0944  data: 0.3300  max mem: 11558\nEpoch: [0]  [ 60/358]  eta: 0:11:00  lr: 0.000858  loss: 0.4740 (1.6080)  loss_classifier: 0.0458 (0.5346)  loss_box_reg: 0.0067 (0.0086)  loss_objectness: 0.1070 (0.3534)  loss_rpn_box_reg: 0.3358 (0.7114)  time: 2.0933  data: 0.3305  max mem: 11558\nEpoch: [0]  [ 70/358]  eta: 0:10:33  lr: 0.000998  loss: 0.3671 (1.4303)  loss_classifier: 0.0366 (0.4647)  loss_box_reg: 0.0051 (0.0080)  loss_objectness: 0.0469 (0.3087)  loss_rpn_box_reg: 0.2796 (0.6489)  time: 2.0946  data: 0.3342  max mem: 11558\nEpoch: [0]  [ 80/358]  eta: 0:10:07  lr: 0.001138  loss: 0.2920 (1.2857)  loss_classifier: 0.0325 (0.4108)  loss_box_reg: 0.0037 (0.0074)  loss_objectness: 0.0250 (0.2728)  loss_rpn_box_reg: 0.2341 (0.5946)  time: 2.0904  data: 0.3311  max mem: 11558\nEpoch: [0]  [ 90/358]  eta: 0:09:43  lr: 0.001278  loss: 0.2291 (1.1693)  loss_classifier: 0.0258 (0.3687)  loss_box_reg: 0.0040 (0.0072)  loss_objectness: 0.0133 (0.2440)  loss_rpn_box_reg: 0.1856 (0.5494)  time: 2.0933  data: 0.3362  max mem: 11558\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_5050/1422458962.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_5050/3257049112.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, data_loader_test)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# train for one epoch, printing every 10 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/torch-vision-references/references/engine.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq, scaler)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[operator]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mobjectness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_bbox_deltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0manchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mnum_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/models/detection/anchor_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image_list, feature_maps)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             ]\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrid_sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         ]\n\u001b[1;32m    129\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_cell_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/models/detection/anchor_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             ]\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrid_sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         ]\n\u001b[1;32m    129\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_cell_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"model.eval()\nfor images, targets in train_data_loader:\n    images = list(image.to(device) for image in images)\n    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n    scaler = None\n    with torch.cuda.amp.autocast(enabled=scaler is not None):\n        loss_dict = model(images, targets)\n        print(loss_dict)\n        losses = sum(loss for loss in loss_dict.values())\n    print(loss_dict)\n    break","metadata":{"execution":{"iopub.status.busy":"2022-12-30T08:00:08.815362Z","iopub.execute_input":"2022-12-30T08:00:08.817052Z","iopub.status.idle":"2022-12-30T08:00:09.886213Z","shell.execute_reply.started":"2022-12-30T08:00:08.817006Z","shell.execute_reply":"2022-12-30T08:00:09.884586Z"},"trusted":true},"execution_count":29,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_5050/1039655855.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     93\u001b[0m                     )\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/models/detection/backbone_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/ops/feature_pyramid_network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mfeat_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner_lateral\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0minner_top_down\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_inner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeat_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nearest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0mlast_inner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner_lateral\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minner_top_down\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result_from_layer_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_inner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 626.00 MiB (GPU 0; 15.90 GiB total capacity; 13.98 GiB already allocated; 151.75 MiB free; 14.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 626.00 MiB (GPU 0; 15.90 GiB total capacity; 13.98 GiB already allocated; 151.75 MiB free; 14.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}