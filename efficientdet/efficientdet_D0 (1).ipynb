{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq1HS5fIFkWS",
        "outputId": "e7513f0b-8d35-480e-b535-22bf17bee11f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX\n",
        "!pip install webcolors"
      ],
      "metadata": {
        "id": "LZI0SwboFmGu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c4577f3-496e-4802-c517-ecd73c882653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (1.21.6)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (3.19.6)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting webcolors\n",
            "  Downloading webcolors-1.12-py3-none-any.whl (9.9 kB)\n",
            "Installing collected packages: webcolors\n",
            "Successfully installed webcolors-1.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cài đặt và import thư viện"
      ],
      "metadata": {
        "id": "m8SHGWaiDYs2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iR5kB01f9x5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "323d5fcd-2232-4c66-d17f-d4fb687a9a73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'voc2coco'...\n",
            "remote: Enumerating objects: 381, done.\u001b[K\n",
            "remote: Counting objects: 100% (381/381), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 381 (delta 359), reused 376 (delta 359), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (381/381), 127.61 KiB | 580.00 KiB/s, done.\n",
            "Resolving deltas: 100% (359/359), done.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "!git clone --depth 1 https://github.com/yukkyo/voc2coco\n",
        "sys.path.append('.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "chuyển directory đang làm việc hiện tại đến 1 đường dẫn"
      ],
      "metadata": {
        "id": "ru5c1z99Dd8b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXagP7Kt-wxr"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/voc2coco')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dowload dữ liệu"
      ],
      "metadata": {
        "id": "DPZ1AqzQDoyr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpMj1uTT_oEp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5feb5020-8707-4f8d-f319-b2091c36a95c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-23 03:21:31--  http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
            "Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\n",
            "Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1999639040 (1.9G) [application/x-tar]\n",
            "Saving to: ‘VOCtrainval_11-May-2012.tar’\n",
            "\n",
            "VOCtrainval_11-May- 100%[===================>]   1.86G  30.2MB/s    in 66s     \n",
            "\n",
            "2022-12-23 03:22:36 (29.1 MB/s) - ‘VOCtrainval_11-May-2012.tar’ saved [1999639040/1999639040]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download and unzip dataset\n",
        "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
        "!tar xf VOCtrainval_11-May-2012.tar"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "chia tập dữ liệu thành 3 phần: train, val, test."
      ],
      "metadata": {
        "id": "F52HSf5KDutS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oqf2lCZV_1Rb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dec6e23-8afe-4385-b288-5ce03897908a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting split-folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.5.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 40076 files [00:10, 3777.66 files/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install split-folders\n",
        "\n",
        "import splitfolders\n",
        "splitfolders.ratio('/content/voc2coco/VOCdevkit/VOC2012', output=\"dataset\", seed=42, ratio=(.7, .1, .2)) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "xoá dữ liệu file .xml trong folder Annotations và tạo lại để thêm dữ liệu file .xml cho bài toán của chúng tôi."
      ],
      "metadata": {
        "id": "oq9IP0o1ELbg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX97ytM7BbJx"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/voc2coco/sample/Annotations\n",
        "!mkdir /content/voc2coco/sample/Annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXqsM25rBr2-"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "path = '/content/voc2coco/VOCdevkit/VOC2012/Annotations'\n",
        "for filename in os.listdir(path):\n",
        "    fullname = os.path.join(path, filename)\n",
        "    shutil.move(fullname, '/content/voc2coco/sample/Annotations') \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRt4k_JUCRyR"
      },
      "outputs": [],
      "source": [
        "with open('/content/voc2coco/sample/dataset_ids/train.txt','w') as f:\n",
        "    list_train = [i[:-4] for i in os.listdir('/content/voc2coco/dataset/train/Annotations')]\n",
        "    for i in list_train:\n",
        "        f.write(i)\n",
        "        f.write('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KG_yxKNIMUxY"
      },
      "outputs": [],
      "source": [
        "with open('/content/voc2coco/sample/dataset_ids/val.txt','w') as f:\n",
        "    list_train = [i[:-4] for i in os.listdir('/content/voc2coco/dataset/val/Annotations')]\n",
        "    for i in list_train:\n",
        "        f.write(i)\n",
        "        f.write('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/voc2coco/sample/dataset_ids/test.txt','w') as f:\n",
        "    list_train = [i[:-4] for i in os.listdir('/content/voc2coco/dataset/test/Annotations')]\n",
        "    for i in list_train:\n",
        "        f.write(i)\n",
        "        f.write('\\n')"
      ],
      "metadata": {
        "id": "UKh7ZlBdHCs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHuo3i5NCuPn"
      },
      "outputs": [],
      "source": [
        "with open('/content/voc2coco/sample/annpaths_list.txt','w') as f:\n",
        "    for i in os.listdir('/content/voc2coco/sample/Annotations'):\n",
        "        f.write('./sample/Annotations/' + i)\n",
        "        f.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "write label to labets.txt"
      ],
      "metadata": {
        "id": "VZlQMQHG5m4S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Wyf4ffCD7xK"
      },
      "outputs": [],
      "source": [
        "object_list = ['aeroplane','bicycle','bird','boat',\\\n",
        "               'bottle','bus','car','cat','chair','cow','diningtable',\\\n",
        "               'dog','horse','motorbike','person'\\\n",
        "               ,'pottedplant','sheep','sofa','train','tvmonitor']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_k9fcTyFP4A"
      },
      "outputs": [],
      "source": [
        "with open('/content/voc2coco/sample/labels.txt','w') as f:\n",
        "    for i in object_list:\n",
        "        f.write(i)\n",
        "        f.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "start convert pascal voc format into coco format."
      ],
      "metadata": {
        "id": "AxoHTC8p44Ct"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q86-I-65FcSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "462d69d6-3a7b-4398-9376-e571a22a7a10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start converting !\n",
            "100% 11987/11987 [00:01<00:00, 10080.50it/s]\n"
          ]
        }
      ],
      "source": [
        "!python voc2coco.py --ann_dir sample/Annotations \\\n",
        "    --ann_ids sample/dataset_ids/train.txt \\\n",
        "    --labels sample/labels.txt \\\n",
        "    --output sample/instances_train.json \\\n",
        "    --ext xml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LA1u4TzLGomT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d18a923-447d-4978-a537-53231476faad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start converting !\n",
            "100% 1712/1712 [00:00<00:00, 7000.12it/s]\n"
          ]
        }
      ],
      "source": [
        "!python voc2coco.py --ann_dir sample/Annotations \\\n",
        "    --ann_ids sample/dataset_ids/val.txt \\\n",
        "    --labels sample/labels.txt \\\n",
        "    --output sample/instances_val.json \\\n",
        "    --ext xml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python voc2coco.py --ann_dir sample/Annotations \\\n",
        "    --ann_ids sample/dataset_ids/test.txt \\\n",
        "    --labels sample/labels.txt \\\n",
        "    --output sample/instances_test.json \\\n",
        "    --ext xml"
      ],
      "metadata": {
        "id": "V7vUziy15Juy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cca6126-2414-43fa-c9e6-c0000789c4a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start converting !\n",
            "100% 3426/3426 [00:00<00:00, 11870.68it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "done for convert voc into pascal voc"
      ],
      "metadata": {
        "id": "7FNcyK-_-kFR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6GDu0WNN685"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfKI3GOxPM-j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a65519c5-80af-4dd7-b465-02bb8e8c6171"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHGCERkuPO31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50e805b0-cb75-45c0-917b-a55f355e2a6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Yet-Another-EfficientDet-Pytorch'...\n",
            "remote: Enumerating objects: 47, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 47 (delta 3), reused 24 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (47/47), done.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys \n",
        "\n",
        "if \"projects\" not in os.getcwd():\n",
        "  !git clone --depth 1 https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch\n",
        "  os.chdir('/content/Yet-Another-EfficientDet-Pytorch')\n",
        "  sys.path.append('.')\n",
        "else:\n",
        "  !git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKi-yIp5PWaK"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/Yet-Another-EfficientDet-Pytorch/datasets\n",
        "!mkdir /content/Yet-Another-EfficientDet-Pytorch/datasets/voc\n",
        "!mkdir /content/Yet-Another-EfficientDet-Pytorch/datasets/voc/train\n",
        "!mkdir /content/Yet-Another-EfficientDet-Pytorch/datasets/voc/val\n",
        "!mkdir /content/Yet-Another-EfficientDet-Pytorch/datasets/voc/test   \n",
        "!mkdir /content/Yet-Another-EfficientDet-Pytorch/datasets/voc/annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-wIiyT4P3Le"
      },
      "outputs": [],
      "source": [
        "path = '/content/voc2coco/dataset/train/JPEGImages'\n",
        "for filename in os.listdir(path):\n",
        "    fullname = os.path.join(path,filename)\n",
        "    shutil.move(fullname, \"/content/Yet-Another-EfficientDet-Pytorch/datasets/voc/train\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3Bjrk5XRa7M"
      },
      "outputs": [],
      "source": [
        "path = '/content/voc2coco/dataset/val/JPEGImages'\n",
        "for filename in os.listdir(path):\n",
        "    fullname = os.path.join(path,filename)\n",
        "    shutil.move(fullname, \"/content/Yet-Another-EfficientDet-Pytorch/datasets/voc/val\") "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/voc2coco/dataset/test/JPEGImages'\n",
        "for filename in os.listdir(path):\n",
        "    fullname = os.path.join(path,filename)\n",
        "    shutil.move(fullname, \"/content/Yet-Another-EfficientDet-Pytorch/datasets/voc/test\") "
      ],
      "metadata": {
        "id": "b6DkdJf7ATVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKY8t5_eR_tL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "d50a580f-de03-41bd-e097-0d89441cde4e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Yet-Another-EfficientDet-Pytorch/datasets/voc/annotations/instances_test.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "path1 = '/content/voc2coco/sample/instances_train.json'\n",
        "path2 = '/content/voc2coco/sample/instances_val.json'\n",
        "path3 = '/content/voc2coco/sample/instances_test.json'\n",
        "\n",
        "shutil.move(path1, '/content/Yet-Another-EfficientDet-Pytorch/datasets/voc/annotations')\n",
        "shutil.move(path2, '/content/Yet-Another-EfficientDet-Pytorch/datasets/voc/annotations')\n",
        "shutil.move(path3, '/content/Yet-Another-EfficientDet-Pytorch/datasets/voc/annotations')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "because giá trị của biến id là string, do đó ta cần chuyển nó về integer"
      ],
      "metadata": {
        "id": "dPA6W0z0BhQG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wr6vkMnF_Rf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba32e047-706d-42d6-c3b7-1940833c698e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['images', 'type', 'annotations', 'categories'])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "import json\n",
        "f = open('/content/Yet-Another-EfficientDet-Pytorch/datasets/voc/annotations/instances_train.json')\n",
        "data = json.load(f)\n",
        "data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fwgFclJGrZW"
      },
      "outputs": [],
      "source": [
        "for i in range(len(data['images'])):\n",
        "      value = data['images'][i]['id']\n",
        "      value1 = value[:4] + value[5:]\n",
        "      data['images'][i]['id'] = int(value1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7REQpi1Vvbw"
      },
      "outputs": [],
      "source": [
        "for i in range(len(data['annotations'])):\n",
        "      value = data['annotations'][i]['image_id']\n",
        "      value1 = value[:4] + value[5:]\n",
        "      data['annotations'][i]['image_id'] = int(value1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tonwvp_eWDui"
      },
      "outputs": [],
      "source": [
        "# data['annotations'][2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a60ayoa0G5iC"
      },
      "outputs": [],
      "source": [
        "# data['annotations'][100]['image_id']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTYHQrp9R4Vt"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/Yet-Another-EfficientDet-Pytorch/datasets/voc/annotations/instances_train.json\n",
        "\n",
        "with open('/content/Yet-Another-EfficientDet-Pytorch/datasets/voc/annotations/instances_train.json', 'w') as f:\n",
        "    f.write(json.dumps(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pm8epMS_TCOW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3baf6ebb-09dd-452c-d6b7-019802c6a96e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['images', 'type', 'annotations', 'categories'])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "f = open('/content/Yet-Another-EfficientDet-Pytorch/datasets/voc/annotations/instances_val.json')\n",
        "data = json.load(f)\n",
        "data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mZJHvzUTJfh"
      },
      "outputs": [],
      "source": [
        "for i in range(len(data['images'])):\n",
        "      value = data['images'][i]['id']\n",
        "      value1 = value[:4] + value[5:]\n",
        "      data['images'][i]['id'] = int(value1)\n",
        "\n",
        "for i in range(len(data['annotations'])):\n",
        "      value = data['annotations'][i]['image_id']\n",
        "      value1 = value[:4] + value[5:]\n",
        "      data['annotations'][i]['image_id'] = int(value1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aX2-GqrzTUS7"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/Yet-Another-EfficientDet-Pytorch/datasets/voc/annotations/instances_val.json\n",
        "\n",
        "with open('/content/Yet-Another-EfficientDet-Pytorch/datasets/voc/annotations/instances_val.json', 'w') as f:\n",
        "    f.write(json.dumps(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHOaDSpHTYMS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f52b8cb-5806-4427-8522-8049fc190b52"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['images', 'type', 'annotations', 'categories'])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "f = open('/content/Yet-Another-EfficientDet-Pytorch/datasets/voc/annotations/instances_test.json')\n",
        "data = json.load(f)\n",
        "data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(data['images'])):\n",
        "      value = data['images'][i]['id']\n",
        "      value1 = value[:4] + value[5:]\n",
        "      data['images'][i]['id'] = int(value1)\n",
        "\n",
        "for i in range(len(data['annotations'])):\n",
        "      value = data['annotations'][i]['image_id']\n",
        "      value1 = value[:4] + value[5:]\n",
        "      data['annotations'][i]['image_id'] = int(value1)"
      ],
      "metadata": {
        "id": "UPJnZKsSChRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/Yet-Another-EfficientDet-Pytorch/datasets/voc/annotations/instances_test.json\n",
        "\n",
        "with open('/content/Yet-Another-EfficientDet-Pytorch/datasets/voc/annotations/instances_test.json', 'w') as f:\n",
        "    f.write(json.dumps(data))"
      ],
      "metadata": {
        "id": "MKcwur_ACjCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.copy(\"/content/gdrive/MyDrive/Colab Notebooks/voc.yml\", \"/content/Yet-Another-EfficientDet-Pytorch/projects\")"
      ],
      "metadata": {
        "id": "zG9oGbAwGuEI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "878fe604-6dbe-45d8-be36-2785cf446c9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Yet-Another-EfficientDet-Pytorch/projects/voc.yml'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "metadata": {
        "id": "2JPp9aJVGxeE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "f8791b28-cb3b-4f8a-edf2-8a4fd1fc114b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Yet-Another-EfficientDet-Pytorch'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ts3-kaANSbBO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49724920-ff9e-439d-e2e4-6385fd03c08c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-23 03:23:08--  https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch/releases/download/1.0/efficientdet-d0.pth\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/253385242/9b9d2100-791d-11ea-80b2-d35899cf95fe?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221223%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221223T032308Z&X-Amz-Expires=300&X-Amz-Signature=4fe60075ab390e12f15534a80d031cb23e6a7bfea285b1cdfc56262f931c7c04&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=253385242&response-content-disposition=attachment%3B%20filename%3Defficientdet-d0.pth&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-12-23 03:23:08--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/253385242/9b9d2100-791d-11ea-80b2-d35899cf95fe?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221223%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221223T032308Z&X-Amz-Expires=300&X-Amz-Signature=4fe60075ab390e12f15534a80d031cb23e6a7bfea285b1cdfc56262f931c7c04&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=253385242&response-content-disposition=attachment%3B%20filename%3Defficientdet-d0.pth&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15862583 (15M) [application/octet-stream]\n",
            "Saving to: ‘weights/efficientdet-d0.pth’\n",
            "\n",
            "weights/efficientde 100%[===================>]  15.13M  31.2MB/s    in 0.5s    \n",
            "\n",
            "2022-12-23 03:23:09 (31.2 MB/s) - ‘weights/efficientdet-d0.pth’ saved [15862583/15862583]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download pretrained weights\n",
        "! mkdir weights\n",
        "! wget https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch/releases/download/1.0/efficientdet-d0.pth -O weights/efficientdet-d0.pth\n",
        "\n",
        "# prepare project file projects/logo.yml\n",
        "# showing its contents here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTWLuhom-qe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dc36877-5e16-4a0f-962d-e82fbac782ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "project_name: voc  # also the folder name of the dataset that under data_path folder\r\n",
            "train_set: train\r\n",
            "val_set: val\r\n",
            "num_gpus: 1\r\n",
            "\r\n",
            "# mean and std in RGB order, actually this part should remain unchanged as long as your dataset is similar to coco.\r\n",
            "mean: [ 0.485, 0.456, 0.406 ]\r\n",
            "std: [ 0.229, 0.224, 0.225 ]\r\n",
            "\r\n",
            "# this anchor is adapted to the dataset\r\n",
            "anchors_scales: '[2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]'\r\n",
            "anchors_ratios: '[(1.0, 1.0), (1.3, 0.8), (1.9, 0.5)]'\r\n",
            "\r\n",
            "obj_list: [ 'aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow','diningtable','dog','horse','motorbike','person','pottedplant','sheep','sofa','train','tvmonitor']"
          ]
        }
      ],
      "source": [
        "!cat projects/voc.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGtMRW0bZsJ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab785bd-699a-4283-fcd9-c8b86c2bbb45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.14s)\n",
            "creating index...\n",
            "index created!\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "loading annotations into memory...\n",
            "Done (t=0.07s)\n",
            "creating index...\n",
            "index created!\n",
            "[Warning] Ignoring Error(s) in loading state_dict for EfficientDetBackbone:\n",
            "\tsize mismatch for classifier.header.pointwise_conv.conv.weight: copying a param with shape torch.Size([810, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([180, 64, 1, 1]).\n",
            "\tsize mismatch for classifier.header.pointwise_conv.conv.bias: copying a param with shape torch.Size([810]) from checkpoint, the shape in current model is torch.Size([180]).\n",
            "[Warning] Don't panic if you see this, this might be because you load a pretrained weights with different number of classes. The rest of the weights should be loaded already.\n",
            "[Info] loaded weights: efficientdet-d0.pth, resuming checkpoint from step: 0\n",
            "[Info] freezed backbone\n",
            "Step: 99. Epoch: 0/2. Iteration: 100/374. Cls loss: 1.67562. Reg loss: 1.19144. Total loss: 2.86706:  26% 99/374 [01:49<03:57,  1.16it/s]checkpoint...\n",
            "Step: 199. Epoch: 0/2. Iteration: 200/374. Cls loss: 1.64936. Reg loss: 1.02549. Total loss: 2.67485:  53% 199/374 [03:15<02:29,  1.17it/s]checkpoint...\n",
            "Step: 299. Epoch: 0/2. Iteration: 300/374. Cls loss: 1.35968. Reg loss: 1.09004. Total loss: 2.44972:  80% 299/374 [04:43<01:04,  1.17it/s]checkpoint...\n",
            "Step: 373. Epoch: 0/2. Iteration: 374/374. Cls loss: 1.17722. Reg loss: 1.26423. Total loss: 2.44145: 100% 374/374 [05:45<00:00,  1.08it/s]\n",
            "Val. Epoch: 0/2. Classification loss: 1.26492. Regression loss: 1.14015. Total loss: 2.40507\n",
            "Step: 399. Epoch: 1/2. Iteration: 26/374. Cls loss: 1.17051. Reg loss: 1.05626. Total loss: 2.22677:   7% 25/374 [00:36<04:59,  1.16it/s]checkpoint...\n",
            "Step: 499. Epoch: 1/2. Iteration: 126/374. Cls loss: 0.91073. Reg loss: 1.39176. Total loss: 2.30249:  33% 125/374 [02:02<03:33,  1.17it/s]checkpoint...\n",
            "Step: 599. Epoch: 1/2. Iteration: 226/374. Cls loss: 1.09304. Reg loss: 1.20417. Total loss: 2.29721:  60% 225/374 [03:28<02:06,  1.17it/s]checkpoint...\n",
            "Step: 699. Epoch: 1/2. Iteration: 326/374. Cls loss: 0.89274. Reg loss: 1.07804. Total loss: 1.97079:  87% 325/374 [04:54<00:41,  1.18it/s]checkpoint...\n",
            "Step: 747. Epoch: 1/2. Iteration: 374/374. Cls loss: 0.74739. Reg loss: 1.33446. Total loss: 2.08185: 100% 374/374 [05:34<00:00,  1.12it/s]\n",
            "Val. Epoch: 1/2. Classification loss: 0.84103. Regression loss: 1.07444. Total loss: 1.91547\n"
          ]
        }
      ],
      "source": [
        "!python train.py -c 0 -p voc --head_only True \\\n",
        "                   --lr 5e-3 --batch_size 32 \\\n",
        "                   --load_weights /content/Yet-Another-EfficientDet-Pytorch/weights/efficientdet-d0.pth  --num_epochs 10 --save_interval 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxV0RKtTgusA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47eb391b-f74d-4831-85b2-c5ae197617eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.17s)\n",
            "creating index...\n",
            "index created!\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "loading annotations into memory...\n",
            "Done (t=0.08s)\n",
            "creating index...\n",
            "index created!\n",
            "using weights logs//voc/efficientdet-d0_1_748.pth\n",
            "[Info] loaded weights: efficientdet-d0_1_748.pth, resuming checkpoint from step: 748\n"
          ]
        }
      ],
      "source": [
        "!python train.py -c 0 -p voc --head_only False --lr 1e-3 --batch_size 32 --load_weights last  --num_epochs 20 --save_interval 100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get latest weight file\n",
        "%cd logs/voc\n",
        "weight_file = !ls -Art | grep efficientdet\n",
        "%cd ../..\n",
        "\n",
        "#uncomment the next line to specify a weight file\n",
        "#weight_file[-1] = 'efficientdet-d2_1_748.pth'\n",
        "\n",
        "!python coco_eval.py -c 0 -p voc -w \"logs/voc/{weight_file[-1]}\""
      ],
      "metadata": {
        "id": "4vOW-nJt1hg2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14a7cea8-e08c-4eff-eb40-126b329d4e28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Yet-Another-EfficientDet-Pytorch/logs/voc\n",
            "/content/Yet-Another-EfficientDet-Pytorch\n",
            "running coco-style evaluation on project voc, weights logs/voc/efficientdet-d0_1_748.pth...\n",
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "100% 1712/1712 [01:46<00:00, 16.00it/s]\n",
            "Loading and preparing results...\n",
            "DONE (t=4.71s)\n",
            "creating index...\n",
            "index created!\n",
            "BBox\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=13.45s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=8.25s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.026\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.048\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.025\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.002\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.007\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.034\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.190\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.308\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.330\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.167\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.412\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## còn phần visualization sample image\n"
      ],
      "metadata": {
        "id": "d0Qbhpt30iS2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "92775aae-71c8-427b-dfa6-07f9e021131b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-e342d58501fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mweight_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'efficientdet-d0_1_748.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logs/voc/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mweight_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1667\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1668\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for EfficientDetBackbone:\n\tsize mismatch for classifier.header.pointwise_conv.conv.weight: copying a param with shape torch.Size([180, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([18, 64, 1, 1]).\n\tsize mismatch for classifier.header.pointwise_conv.conv.bias: copying a param with shape torch.Size([180]) from checkpoint, the shape in current model is torch.Size([18])."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mg4MPrFIF_7h"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}