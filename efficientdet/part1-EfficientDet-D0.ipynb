{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP5fDd0rbA0wWmRhpRXuanK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gx1RKr3Dqu0H","executionInfo":{"status":"ok","timestamp":1672369350724,"user_tz":-420,"elapsed":26136,"user":{"displayName":"trung nguyen nho","userId":"16795874157616798231"}},"outputId":"7be923ac-561c-49ec-ee70-f0400b0abfea"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","source":["# I. Install requirements and import library\n"],"metadata":{"id":"I2mD1k4trukK"}},{"cell_type":"code","source":["!pip install tensorboardX\n","!pip install webcolors\n","!pip install split-folders"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"itV42my0rHPv","executionInfo":{"status":"ok","timestamp":1672369362720,"user_tz":-420,"elapsed":12002,"user":{"displayName":"trung nguyen nho","userId":"16795874157616798231"}},"outputId":"72803ad8-0ff1-4351-b8f1-c875a2f9b3e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorboardX\n","  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 36.8 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (3.19.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (1.21.6)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.5.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting webcolors\n","  Downloading webcolors-1.12-py3-none-any.whl (9.9 kB)\n","Installing collected packages: webcolors\n","Successfully installed webcolors-1.12\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting split-folders\n","  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n","Installing collected packages: split-folders\n","Successfully installed split-folders-0.5.1\n"]}]},{"cell_type":"code","source":["import os\n","import sys\n","import splitfolders\n","import shutil\n","import json"],"metadata":{"id":"tttTl__Ms9Qj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# II. Split training, val, test set and convert PASCAL VOC format into COCO format."],"metadata":{"id":"Rkaw2PNCtRxr"}},{"cell_type":"markdown","source":["clone tool for convert pascal voc into coco format"],"metadata":{"id":"GuVWlHtzth4r"}},{"cell_type":"code","source":["# Download source code.\n","if \"voc2coco\" not in os.getcwd():\n","  !git clone --depth 1 https://github.com/yukkyo/voc2coco\n","  os.chdir('/content/voc2coco')\n","  sys.path.append('.')\n","else:\n","  !git pull"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GJXVixgLthaV","executionInfo":{"status":"ok","timestamp":1672369364639,"user_tz":-420,"elapsed":1926,"user":{"displayName":"trung nguyen nho","userId":"16795874157616798231"}},"outputId":"64bf3cfe-1b2f-4f74-8ff0-996248b14da8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'voc2coco'...\n","remote: Enumerating objects: 381, done.\u001b[K\n","remote: Counting objects: 100% (381/381), done.\u001b[K\n","remote: Compressing objects: 100% (21/21), done.\u001b[K\n","remote: Total 381 (delta 359), reused 376 (delta 359), pack-reused 0\u001b[K\n","Receiving objects: 100% (381/381), 127.61 KiB | 18.23 MiB/s, done.\n","Resolving deltas: 100% (359/359), done.\n"]}]},{"cell_type":"markdown","source":["## 1.Download and split data"],"metadata":{"id":"DjS8FoJ1u3m7"}},{"cell_type":"markdown","source":["download data"],"metadata":{"id":"QuXY3u2iuUe-"}},{"cell_type":"code","source":["# download and unzip dataset\n","!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n","!tar xf VOCtrainval_11-May-2012.tar"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ATqRxKCstQ_K","executionInfo":{"status":"ok","timestamp":1672369510322,"user_tz":-420,"elapsed":145691,"user":{"displayName":"trung nguyen nho","userId":"16795874157616798231"}},"outputId":"0e9095d0-3877-4b68-87fa-2f7a4aee982e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-12-30 03:02:41--  http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n","Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\n","Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1999639040 (1.9G) [application/x-tar]\n","Saving to: ‘VOCtrainval_11-May-2012.tar’\n","\n","VOCtrainval_11-May- 100%[===================>]   1.86G  14.6MB/s    in 2m 17s  \n","\n","2022-12-30 03:04:59 (14.0 MB/s) - ‘VOCtrainval_11-May-2012.tar’ saved [1999639040/1999639040]\n","\n"]}]},{"cell_type":"markdown","source":["split data into 3 part: training set, validation set and test set"],"metadata":{"id":"WeZRdnWXuozk"}},{"cell_type":"code","source":["splitfolders.ratio('/content/voc2coco/VOCdevkit/VOC2012', output=\"dataset\", seed=42, ratio=(.6, .2, .2)) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Dk55vZGui66","executionInfo":{"status":"ok","timestamp":1672369521301,"user_tz":-420,"elapsed":11000,"user":{"displayName":"trung nguyen nho","userId":"16795874157616798231"}},"outputId":"e31d4cfd-b9c6-4072-a69b-57273851ece8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Copying files: 40076 files [00:10, 3812.53 files/s]\n"]}]},{"cell_type":"markdown","source":["## 2. Convert PASCAL VOC format into COCO format.\n","\n","\n","To use tools , we need to reformat the structure to use tools, see detail in here: https://github.com/yukkyo/voc2coco\n"],"metadata":{"id":"zVhoQ7DhvP35"}},{"cell_type":"code","source":["!rm -rf /content/voc2coco/sample/Annotations\n","!mkdir /content/voc2coco/sample/Annotations"],"metadata":{"id":"r4hv8Azpuz5_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["move file and write .txt file"],"metadata":{"id":"WBrZhNUFw87r"}},{"cell_type":"code","source":["path = '/content/voc2coco/VOCdevkit/VOC2012/Annotations'\n","for filename in os.listdir(path):\n","    fullname = os.path.join(path, filename)\n","    shutil.move(fullname, '/content/voc2coco/sample/Annotations')   "],"metadata":{"id":"WzYM6s8BwjH5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/voc2coco/sample/dataset_ids/train.txt','w') as f:\n","    list_train = [i[:-4] for i in os.listdir('/content/voc2coco/dataset/train/Annotations')]\n","    for i in list_train:\n","        f.write(i)\n","        f.write('\\n')\n","\n","with open('/content/voc2coco/sample/dataset_ids/val.txt','w') as f:\n","    list_train = [i[:-4] for i in os.listdir('/content/voc2coco/dataset/val/Annotations')]\n","    for i in list_train:\n","        f.write(i)\n","        f.write('\\n')\n","\n","with open('/content/voc2coco/sample/dataset_ids/test.txt','w') as f:\n","    list_train = [i[:-4] for i in os.listdir('/content/voc2coco/dataset/test/Annotations')]\n","    for i in list_train:\n","        f.write(i)\n","        f.write('\\n')\n","\n","with open('/content/voc2coco/sample/annpaths_list.txt','w') as f:\n","    for i in os.listdir('/content/voc2coco/sample/Annotations'):\n","        f.write('./sample/Annotations/' + i)\n","        f.write('\\n')\n","\n","with open('/content/voc2coco/sample/annpaths_list.txt','w') as f:\n","    for i in os.listdir('/content/voc2coco/sample/Annotations'):\n","        f.write('./sample/Annotations/' + i)\n","        f.write('\\n')"],"metadata":{"id":"8kMp7KmCxCqG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["write label to labets.txt"],"metadata":{"id":"2btGG2ICxQE9"}},{"cell_type":"code","source":["object_list = ['aeroplane','bicycle','bird','boat',\\\n","               'bottle','bus','car','cat','chair','cow','diningtable',\\\n","               'dog','horse','motorbike','person'\\\n","               ,'pottedplant','sheep','sofa','train','tvmonitor']\n","\n","\n","with open('/content/voc2coco/sample/labels.txt','w') as f:\n","    for i in object_list:\n","        f.write(i)\n","        f.write('\\n')"],"metadata":{"id":"StsDXeNAxPWq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["start convert pascal voc format into coco format."],"metadata":{"id":"L-M_bo7uxb9t"}},{"cell_type":"code","source":["# convert for training set\n","!python voc2coco.py --ann_dir sample/Annotations \\\n","    --ann_ids sample/dataset_ids/train.txt \\\n","    --labels sample/labels.txt \\\n","    --output sample/instances_train.json \\\n","    --ext xml"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dcyz0ayjxWRF","executionInfo":{"status":"ok","timestamp":1672369524020,"user_tz":-420,"elapsed":2063,"user":{"displayName":"trung nguyen nho","userId":"16795874157616798231"}},"outputId":"3ecbe150-471c-4697-aa34-fba55b9b07fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Start converting !\n","100% 10275/10275 [00:01<00:00, 9900.54it/s]\n"]}]},{"cell_type":"code","source":["# convert for validation set \n","!python voc2coco.py --ann_dir sample/Annotations \\\n","    --ann_ids sample/dataset_ids/val.txt \\\n","    --labels sample/labels.txt \\\n","    --output sample/instances_val.json \\\n","    --ext xml"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JZK5db9AxjNg","executionInfo":{"status":"ok","timestamp":1672369524021,"user_tz":-420,"elapsed":23,"user":{"displayName":"trung nguyen nho","userId":"16795874157616798231"}},"outputId":"e92f75f3-ed4e-4018-af27-bf9041e7352f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Start converting !\n","100% 3425/3425 [00:00<00:00, 13324.66it/s]\n"]}]},{"cell_type":"code","source":["# convert for test set\n","!python voc2coco.py --ann_dir sample/Annotations \\\n","    --ann_ids sample/dataset_ids/test.txt \\\n","    --labels sample/labels.txt \\\n","    --output sample/instances_test.json \\\n","    --ext xml"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p_cXT0hcxnTI","executionInfo":{"status":"ok","timestamp":1672369524022,"user_tz":-420,"elapsed":20,"user":{"displayName":"trung nguyen nho","userId":"16795874157616798231"}},"outputId":"51d9576d-6560-49e6-b682-44110ab87903"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Start converting !\n","100% 3425/3425 [00:00<00:00, 12912.75it/s]\n"]}]},{"cell_type":"markdown","source":["we done convert format."],"metadata":{"id":"fL-loH9yyFya"}},{"cell_type":"markdown","source":["# III. Clone tool for training model and processing for right format.\n","\n","See detail: https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch"],"metadata":{"id":"mizcdNtqy2iL"}},{"cell_type":"code","source":["os.chdir('/content')\n","\n","if \"projects\" not in os.getcwd():\n","  !git clone --depth 1 https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch\n","  os.chdir('/content/Yet-Another-EfficientDet-Pytorch')\n","  sys.path.append('.')\n","else:\n","  !git pull"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sM46ILBcxs2M","executionInfo":{"status":"ok","timestamp":1672369526725,"user_tz":-420,"elapsed":2717,"user":{"displayName":"trung nguyen nho","userId":"16795874157616798231"}},"outputId":"b5c3ddcc-b19f-49dc-9b19-859495ce8789"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Yet-Another-EfficientDet-Pytorch'...\n","remote: Enumerating objects: 47, done.\u001b[K\n","remote: Counting objects: 100% (47/47), done.\u001b[K\n","remote: Compressing objects: 100% (44/44), done.\u001b[K\n","remote: Total 47 (delta 3), reused 24 (delta 1), pack-reused 0\u001b[K\n","Unpacking objects: 100% (47/47), done.\n"]}]},{"cell_type":"code","source":["!mkdir /content/Yet-Another-EfficientDet-Pytorch/datasets\n","!mkdir /content/Yet-Another-EfficientDet-Pytorch/datasets/voc\n","!mkdir /content/Yet-Another-EfficientDet-Pytorch/datasets/voc/train\n","!mkdir /content/Yet-Another-EfficientDet-Pytorch/datasets/voc/val\n","!mkdir /content/Yet-Another-EfficientDet-Pytorch/datasets/voc/test   \n","!mkdir /content/Yet-Another-EfficientDet-Pytorch/datasets/voc/annotations"],"metadata":{"id":"joeYe6JJzTtv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["move the file to the right location"],"metadata":{"id":"sRJMW755zZHp"}},{"cell_type":"code","source":["path = '/content/voc2coco/dataset/train/JPEGImages'\n","for filename in os.listdir(path):\n","    fullname = os.path.join(path,filename)\n","    shutil.move(fullname, \"/content/Yet-Another-EfficientDet-Pytorch/datasets/voc/train\") \n","\n","path = '/content/voc2coco/dataset/val/JPEGImages'\n","for filename in os.listdir(path):\n","    fullname = os.path.join(path,filename)\n","    shutil.move(fullname, \"/content/Yet-Another-EfficientDet-Pytorch/datasets/voc/val\") \n","\n","path = '/content/voc2coco/dataset/test/JPEGImages'\n","for filename in os.listdir(path):\n","    fullname = os.path.join(path,filename)\n","    shutil.move(fullname, \"/content/Yet-Another-EfficientDet-Pytorch/datasets/voc/test\") \n","\n","path1 = '/content/voc2coco/sample/instances_train.json'\n","path2 = '/content/voc2coco/sample/instances_val.json'\n","path3 = '/content/voc2coco/sample/instances_test.json'\n","\n","shutil.move(path1, '/content/Yet-Another-EfficientDet-Pytorch/datasets/voc/annotations')\n","shutil.move(path2, '/content/Yet-Another-EfficientDet-Pytorch/datasets/voc/annotations')\n","shutil.move(path3, '/content/Yet-Another-EfficientDet-Pytorch/datasets/voc/annotations')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"LRpL9BmxzYZe","executionInfo":{"status":"ok","timestamp":1672369528893,"user_tz":-420,"elapsed":1528,"user":{"displayName":"trung nguyen nho","userId":"16795874157616798231"}},"outputId":"39bac99a-c129-497c-ff4d-85f390c283ca"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/Yet-Another-EfficientDet-Pytorch/datasets/voc/annotations/instances_test.json'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["because the type of id is string, so we need to convert it into integer."],"metadata":{"id":"u7Vtwe16zsCY"}},{"cell_type":"code","source":["def convert_id(path):\n","  f = open(path)\n","  data = json.load(f)\n","\n","  for i in range(len(data['images'])):\n","      value = data['images'][i]['id']\n","      value1 = value[:4] + value[5:]\n","      data['images'][i]['id'] = int(value1)\n","  \n","  for i in range(len(data['annotations'])):\n","      value = data['annotations'][i]['image_id']\n","      value1 = value[:4] + value[5:]\n","      data['annotations'][i]['image_id'] = int(value1)\n","\n","  !rm -rf path\n","  with open(path, 'w') as f:\n","    f.write(json.dumps(data))"],"metadata":{"id":"QZnZHbjxzqvr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for traning, validation and test set:\n","convert_id('/content/Yet-Another-EfficientDet-Pytorch/datasets/voc/annotations/instances_train.json')\n","convert_id('/content/Yet-Another-EfficientDet-Pytorch/datasets/voc/annotations/instances_val.json')\n","convert_id('/content/Yet-Another-EfficientDet-Pytorch/datasets/voc/annotations/instances_test.json')"],"metadata":{"id":"tbMTdsM50var"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# download pretrained weights\n","! mkdir weights\n","! wget https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch/releases/download/1.0/efficientdet-d0.pth -O weights/efficientdet-d0.pth"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MIB7HNkT1NqL","executionInfo":{"status":"ok","timestamp":1672369538279,"user_tz":-420,"elapsed":8673,"user":{"displayName":"trung nguyen nho","userId":"16795874157616798231"}},"outputId":"a55e865a-0831-4f35-ae5b-9988fe37120b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-12-30 03:05:26--  https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch/releases/download/1.0/efficientdet-d0.pth\n","Resolving github.com (github.com)... 20.205.243.166\n","Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/253385242/9b9d2100-791d-11ea-80b2-d35899cf95fe?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221230%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221230T030526Z&X-Amz-Expires=300&X-Amz-Signature=b3a550e14d48bf5d3ec349f91a303f55d31f6f62893d6e8539daa23b0b3fb2d3&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=253385242&response-content-disposition=attachment%3B%20filename%3Defficientdet-d0.pth&response-content-type=application%2Foctet-stream [following]\n","--2022-12-30 03:05:26--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/253385242/9b9d2100-791d-11ea-80b2-d35899cf95fe?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221230%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221230T030526Z&X-Amz-Expires=300&X-Amz-Signature=b3a550e14d48bf5d3ec349f91a303f55d31f6f62893d6e8539daa23b0b3fb2d3&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=253385242&response-content-disposition=attachment%3B%20filename%3Defficientdet-d0.pth&response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 15862583 (15M) [application/octet-stream]\n","Saving to: ‘weights/efficientdet-d0.pth’\n","\n","weights/efficientde 100%[===================>]  15.13M  6.69MB/s    in 2.3s    \n","\n","2022-12-30 03:05:29 (6.69 MB/s) - ‘weights/efficientdet-d0.pth’ saved [15862583/15862583]\n","\n"]}]},{"cell_type":"markdown","source":["we have voc.yml file: this file have some parameter."],"metadata":{"id":"ZW9rwAOl1aGf"}},{"cell_type":"code","source":["shutil.copy(\"/content/gdrive/MyDrive/Colab Notebooks/voc.yml\", \"/content/Yet-Another-EfficientDet-Pytorch/projects\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"vWJ2y9WO1VjV","executionInfo":{"status":"ok","timestamp":1672369538282,"user_tz":-420,"elapsed":55,"user":{"displayName":"trung nguyen nho","userId":"16795874157616798231"}},"outputId":"a9960790-511b-46ad-828c-0fc3e3e99f37"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/Yet-Another-EfficientDet-Pytorch/projects/voc.yml'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["#showing its contents here\n","!cat projects/voc.yml "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-WvSsHFW1jAw","executionInfo":{"status":"ok","timestamp":1672369538285,"user_tz":-420,"elapsed":56,"user":{"displayName":"trung nguyen nho","userId":"16795874157616798231"}},"outputId":"28d4ff3e-e21d-4119-fe25-ffb7c9b7dd45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["project_name: voc  # also the folder name of the dataset that under data_path folder\r\n","train_set: train\r\n","val_set: val\r\n","num_gpus: 1\r\n","\r\n","# mean and std in RGB order, actually this part should remain unchanged as long as your dataset is similar to coco.\r\n","mean: [ 0.485, 0.456, 0.406 ]\r\n","std: [ 0.229, 0.224, 0.225 ]\r\n","\r\n","# this anchor is adapted to the dataset\r\n","anchors_scales: '[2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]'\r\n","anchors_ratios: '[(1.0, 1.0), (1.3, 0.8), (1.9, 0.5)]'\r\n","\r\n","obj_list: [ 'aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow','diningtable','dog','horse','motorbike','person','pottedplant','sheep','sofa','train','tvmonitor']"]}]},{"cell_type":"markdown","source":["# IV.Training: use pre-trained model in COCO dataset"],"metadata":{"id":"11QCZxv_1vxG"}},{"cell_type":"markdown","source":["we train the model with lr=4e-3 for the first 15 epochs and lr=e-3 for the later 15 epochs. Since I don't have enough resources, then 15 epochs will be trained and the model evaluation will be in another file."],"metadata":{"id":"h3QTwCKc9fkd"}},{"cell_type":"code","source":["!python train.py -c 0 -p voc --head_only True \\\n","                   --lr 4e-3 --batch_size 32 \\\n","                   --load_weights /content/Yet-Another-EfficientDet-Pytorch/weights/efficientdet-d0.pth  --num_epochs 15 --save_interval 300"],"metadata":{"id":"fwJVgbJP1nSM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672375072465,"user_tz":-420,"elapsed":5534231,"user":{"displayName":"trung nguyen nho","userId":"16795874157616798231"}},"outputId":"f537f2f3-4f9d-49c5-b74c-e2910a614f81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["loading annotations into memory...\n","Done (t=0.12s)\n","creating index...\n","index created!\n","/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","loading annotations into memory...\n","Done (t=0.08s)\n","creating index...\n","index created!\n","[Warning] Ignoring Error(s) in loading state_dict for EfficientDetBackbone:\n","\tsize mismatch for classifier.header.pointwise_conv.conv.weight: copying a param with shape torch.Size([810, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([180, 64, 1, 1]).\n","\tsize mismatch for classifier.header.pointwise_conv.conv.bias: copying a param with shape torch.Size([810]) from checkpoint, the shape in current model is torch.Size([180]).\n","[Warning] Don't panic if you see this, this might be because you load a pretrained weights with different number of classes. The rest of the weights should be loaded already.\n","[Info] loaded weights: efficientdet-d0.pth, resuming checkpoint from step: 0\n","[Info] freezed backbone\n","Step: 299. Epoch: 0/15. Iteration: 300/321. Cls loss: 1.93630. Reg loss: 1.01376. Total loss: 2.95006:  93% 299/321 [04:45<00:17,  1.22it/s]checkpoint...\n","Step: 320. Epoch: 0/15. Iteration: 321/321. Cls loss: 1.66351. Reg loss: 1.53505. Total loss: 3.19856: 100% 321/321 [05:02<00:00,  1.06it/s]\n","Val. Epoch: 0/15. Classification loss: 2.00160. Regression loss: 1.16192. Total loss: 3.16352\n","Step: 599. Epoch: 1/15. Iteration: 279/321. Cls loss: 1.48797. Reg loss: 1.10627. Total loss: 2.59424:  87% 278/321 [04:16<00:36,  1.17it/s]checkpoint...\n","Step: 641. Epoch: 1/15. Iteration: 321/321. Cls loss: 1.67593. Reg loss: 0.86000. Total loss: 2.53593: 100% 321/321 [04:51<00:00,  1.10it/s]\n","Val. Epoch: 1/15. Classification loss: 1.36211. Regression loss: 1.06564. Total loss: 2.42776\n","Step: 899. Epoch: 2/15. Iteration: 258/321. Cls loss: 1.00711. Reg loss: 1.16208. Total loss: 2.16919:  80% 257/321 [03:54<00:54,  1.18it/s]checkpoint...\n","Step: 962. Epoch: 2/15. Iteration: 321/321. Cls loss: 0.96112. Reg loss: 1.22049. Total loss: 2.18160: 100% 321/321 [04:48<00:00,  1.11it/s]\n","Val. Epoch: 2/15. Classification loss: 1.05784. Regression loss: 1.08975. Total loss: 2.14759\n","Step: 1199. Epoch: 3/15. Iteration: 237/321. Cls loss: 0.91014. Reg loss: 1.43277. Total loss: 2.34291:  74% 236/321 [03:40<01:12,  1.17it/s]checkpoint...\n","Step: 1283. Epoch: 3/15. Iteration: 321/321. Cls loss: 1.13603. Reg loss: 1.13760. Total loss: 2.27363: 100% 321/321 [04:50<00:00,  1.10it/s]\n","Val. Epoch: 3/15. Classification loss: 0.89172. Regression loss: 1.08929. Total loss: 1.98102\n","Step: 1499. Epoch: 4/15. Iteration: 216/321. Cls loss: 0.90894. Reg loss: 1.18365. Total loss: 2.09259:  67% 215/321 [03:19<01:35,  1.10it/s]checkpoint...\n","Step: 1604. Epoch: 4/15. Iteration: 321/321. Cls loss: 0.88352. Reg loss: 1.09007. Total loss: 1.97359: 100% 321/321 [04:49<00:00,  1.11it/s]\n","Val. Epoch: 4/15. Classification loss: 0.78643. Regression loss: 1.03995. Total loss: 1.82638\n","Step: 1799. Epoch: 5/15. Iteration: 195/321. Cls loss: 0.73787. Reg loss: 1.43697. Total loss: 2.17484:  60% 194/321 [03:00<01:48,  1.17it/s]checkpoint...\n","Step: 1925. Epoch: 5/15. Iteration: 321/321. Cls loss: 0.70320. Reg loss: 1.13494. Total loss: 1.83814: 100% 321/321 [04:47<00:00,  1.12it/s]\n","Val. Epoch: 5/15. Classification loss: 0.71688. Regression loss: 1.08609. Total loss: 1.80297\n","Step: 2099. Epoch: 6/15. Iteration: 174/321. Cls loss: 0.65617. Reg loss: 0.92922. Total loss: 1.58539:  54% 173/321 [02:44<02:24,  1.02it/s]checkpoint...\n","Step: 2246. Epoch: 6/15. Iteration: 321/321. Cls loss: 0.61180. Reg loss: 0.96297. Total loss: 1.57478: 100% 321/321 [04:49<00:00,  1.11it/s]\n","Val. Epoch: 6/15. Classification loss: 0.65833. Regression loss: 1.10603. Total loss: 1.76436\n","Step: 2399. Epoch: 7/15. Iteration: 153/321. Cls loss: 0.72912. Reg loss: 1.25974. Total loss: 1.98886:  47% 152/321 [02:25<02:22,  1.18it/s]checkpoint...\n","Step: 2567. Epoch: 7/15. Iteration: 321/321. Cls loss: 0.66284. Reg loss: 1.16921. Total loss: 1.83205: 100% 321/321 [04:49<00:00,  1.11it/s]\n","Val. Epoch: 7/15. Classification loss: 0.62150. Regression loss: 1.04770. Total loss: 1.66920\n","Step: 2699. Epoch: 8/15. Iteration: 132/321. Cls loss: 0.59846. Reg loss: 1.28585. Total loss: 1.88432:  41% 131/321 [02:06<02:49,  1.12it/s]checkpoint...\n","Step: 2888. Epoch: 8/15. Iteration: 321/321. Cls loss: 0.58617. Reg loss: 1.09073. Total loss: 1.67691: 100% 321/321 [04:49<00:00,  1.11it/s]\n","Val. Epoch: 8/15. Classification loss: 0.57697. Regression loss: 1.05060. Total loss: 1.62757\n","Step: 2999. Epoch: 9/15. Iteration: 111/321. Cls loss: 0.57303. Reg loss: 0.66347. Total loss: 1.23650:  34% 110/321 [01:49<03:00,  1.17it/s]checkpoint...\n","Step: 3209. Epoch: 9/15. Iteration: 321/321. Cls loss: 0.47127. Reg loss: 0.93427. Total loss: 1.40554: 100% 321/321 [04:50<00:00,  1.10it/s]\n","Val. Epoch: 9/15. Classification loss: 0.55009. Regression loss: 1.02647. Total loss: 1.57656\n","Step: 3299. Epoch: 10/15. Iteration: 90/321. Cls loss: 0.54322. Reg loss: 1.13188. Total loss: 1.67510:  28% 89/321 [01:30<03:28,  1.11it/s]checkpoint...\n","Step: 3530. Epoch: 10/15. Iteration: 321/321. Cls loss: 0.57174. Reg loss: 1.32319. Total loss: 1.89493: 100% 321/321 [04:49<00:00,  1.11it/s]\n","Val. Epoch: 10/15. Classification loss: 0.52022. Regression loss: 1.02109. Total loss: 1.54131\n","Step: 3599. Epoch: 11/15. Iteration: 69/321. Cls loss: 0.74180. Reg loss: 1.18413. Total loss: 1.92593:  21% 68/321 [01:12<03:36,  1.17it/s]checkpoint...\n","Step: 3851. Epoch: 11/15. Iteration: 321/321. Cls loss: 0.54094. Reg loss: 1.08657. Total loss: 1.62751: 100% 321/321 [04:48<00:00,  1.11it/s]\n","Val. Epoch: 11/15. Classification loss: 0.50200. Regression loss: 1.03505. Total loss: 1.53705\n","Step: 3899. Epoch: 12/15. Iteration: 48/321. Cls loss: 0.48247. Reg loss: 1.27780. Total loss: 1.76028:  15% 47/321 [00:53<03:56,  1.16it/s]checkpoint...\n","Step: 4172. Epoch: 12/15. Iteration: 321/321. Cls loss: 0.49516. Reg loss: 1.14103. Total loss: 1.63619: 100% 321/321 [04:49<00:00,  1.11it/s]\n","Val. Epoch: 12/15. Classification loss: 0.47306. Regression loss: 1.01824. Total loss: 1.49130\n","Step: 4199. Epoch: 13/15. Iteration: 27/321. Cls loss: 0.44266. Reg loss: 0.83834. Total loss: 1.28100:   8% 26/321 [00:35<04:15,  1.16it/s]checkpoint...\n","Step: 4493. Epoch: 13/15. Iteration: 321/321. Cls loss: 0.42212. Reg loss: 1.10806. Total loss: 1.53018: 100% 321/321 [04:48<00:00,  1.11it/s]\n","Val. Epoch: 13/15. Classification loss: 0.44304. Regression loss: 1.02867. Total loss: 1.47171\n","Step: 4499. Epoch: 14/15. Iteration: 6/321. Cls loss: 0.40320. Reg loss: 1.13700. Total loss: 1.54021:   2% 5/321 [00:17<10:45,  2.04s/it]checkpoint...\n","Step: 4799. Epoch: 14/15. Iteration: 306/321. Cls loss: 0.44410. Reg loss: 1.18231. Total loss: 1.62641:  95% 305/321 [04:37<00:12,  1.28it/s]checkpoint...\n","Step: 4814. Epoch: 14/15. Iteration: 321/321. Cls loss: 0.43791. Reg loss: 1.03428. Total loss: 1.47220: 100% 321/321 [04:49<00:00,  1.11it/s]\n","Val. Epoch: 14/15. Classification loss: 0.45729. Regression loss: 1.04353. Total loss: 1.50083\n"]}]},{"cell_type":"markdown","source":["c : compoud coefficient \n","\n","p: name project\n","\n","head_only: whether finetunes only the regressor and the classifie\n","\n","lr: learning rate\n","\n","load_weights: load pre-trained model in COCO dataset\n","\n","save_interval: Number of steps between saving'"],"metadata":{"id":"QYVBJrYQ120U"}},{"cell_type":"markdown","source":["we save the last checkpoint to continue training 15 epochs later on another\n","file."],"metadata":{"id":"A9pZgxtM9D2r"}},{"cell_type":"code","source":[],"metadata":{"id":"1S6ZXJnl_GMz"},"execution_count":null,"outputs":[]}]}